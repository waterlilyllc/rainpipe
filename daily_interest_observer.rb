#!/usr/bin/env ruby

require 'json'
require 'net/http'
require 'uri'
require 'date'
require 'time'
require 'fileutils'
require 'digest'
require_relative 'interest_manager'
require_relative 'interest_scorer'

class DailyInterestObserver
  GOOGLE_API_URL = 'https://www.googleapis.com/customsearch/v1'
  OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions'
  OUTPUT_DIR = './data/daily_observations'
  CACHE_DIR = './data/observation_cache'
  
  def initialize
    @google_api_key = ENV['GOOGLE_API_KEY']
    @google_cx = ENV['GOOGLE_CUSTOM_SEARCH_CX']
    @openai_api_key = ENV['OPENAI_API_KEY']
    @model = ENV['GPT_MODEL'] || 'gpt-4o-mini'
    
    FileUtils.mkdir_p(OUTPUT_DIR)
    FileUtils.mkdir_p(CACHE_DIR)
    
    @scorer = InterestScorer.new
    @seen_articles_file = File.join(CACHE_DIR, 'seen_articles.json')
    @seen_articles = load_seen_articles
  end
  
  def run_daily_observation
    puts "ğŸ” é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ã®å®šç‚¹è¦³æ¸¬ã‚’é–‹å§‹ã—ã¾ã™..."
    puts "å®Ÿè¡Œæ™‚åˆ»: #{Time.now.strftime('%Y-%m-%d %H:%M:%S')}"
    
    # ã‚¹ã‚³ã‚¢ã®é«˜ã„é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
    scored_interests = @scorer.calculate_scores
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã§ã‚‚å·¡å›ï¼‰
    default_keywords = [
      {
        keyword: 'Claude',
        category: 'ai-ml',
        total_score: 9.0,
        context: 'Anthropicã®å¯¾è©±å‹AIã€‚é–‹ç™ºæ”¯æ´ã‚„å‰µé€ çš„ã‚¿ã‚¹ã‚¯ã§é«˜ã„è©•ä¾¡',
        related_hot_words: [
          { 'word' => 'Claude Code', 'reason' => 'AIé§†å‹•ã®é–‹ç™ºç’°å¢ƒã¨ã—ã¦æ³¨ç›®' },
          { 'word' => 'Opus 4', 'reason' => 'æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Š' }
        ]
      },
      {
        keyword: 'Gemini CLI',
        category: 'ai-ml',
        total_score: 8.5,
        context: 'Googleã®æ–°ã—ã„AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ„ãƒ¼ãƒ«ã€‚é–‹ç™ºè€…ã®ç”Ÿç”£æ€§å‘ä¸Šã«æ³¨ç›®',
        related_hot_words: [
          { 'word' => 'MCP', 'reason' => 'Model Context Protocolå¯¾å¿œã§ä»–ãƒ„ãƒ¼ãƒ«ã¨ã®é€£æºãŒå¯èƒ½' },
          { 'word' => 'AI Agent', 'reason' => 'ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ä½¿ãˆã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä»£è¡¨ä¾‹' }
        ]
      },
      {
        keyword: 'AIé–‹ç™º',
        category: 'technology',
        total_score: 8.0,
        context: 'AIé–¢é€£ã®æœ€æ–°é–‹ç™ºå‹•å‘ã€ãƒ„ãƒ¼ãƒ«ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯',
        related_hot_words: [
          { 'word' => 'LLM', 'reason' => 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–' },
          { 'word' => 'RAG', 'reason' => 'æ¤œç´¢æ‹¡å¼µç”Ÿæˆã®å®Ÿç”¨åŒ–' }
        ]
      },
      {
        keyword: 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°',
        category: 'technology',
        total_score: 7.5,
        context: 'æ–°ã—ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€é–‹ç™ºæ‰‹æ³•',
        related_hot_words: [
          { 'word' => 'Rust', 'reason' => 'ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã§æ³¨ç›®ã®è¨€èª' },
          { 'word' => 'TypeScript', 'reason' => 'JavaScripté–‹ç™ºã®æ¨™æº–ã«' }
        ]
      }
    ]
    
    # é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯æ¥­ç•Œãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—
    if scored_interests.empty?
      puts "âš ï¸  é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¥­ç•Œãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ã—ã¾ã™..."
      
      # æœ€æ–°ã®æ¥­ç•Œãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèª
      trends_file = './data/tech_trends/latest.json'
      if File.exist?(trends_file) && File.mtime(trends_file) > (Time.now - 7*24*60*60)
        # 1é€±é–“ä»¥å†…ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        puts "ğŸ“Š ä¿å­˜æ¸ˆã¿ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ä½¿ç”¨"
        trends_data = JSON.parse(File.read(trends_file))
      else
        # æ–°ã—ããƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—
        puts "ğŸ”„ æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ä¸­..."
        require_relative 'fetch_tech_trends'
        fetcher = TechTrendsFetcher.new
        fetcher.fetch_programming_trends
        
        # å–å¾—ã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰ã‚’èª­ã¿è¾¼ã¿
        if File.exist?(trends_file)
          trends_data = JSON.parse(File.read(trends_file))
        else
          trends_data = nil
        end
      end
      
      # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¯ãƒ¼ãƒ‰ã‚’è¦³æ¸¬ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
      if trends_data && trends_data['trends']
        trend_keywords = trends_data['trends'].first(5).map do |trend|
          {
            keyword: trend['keyword'],
            category: trend['category'] || 'technology',
            total_score: trend['importance'] || 7.0,
            context: trend['reason'],
            related_hot_words: (trend['related_topics'] || []).first(3).map do |topic|
              { 'word' => topic, 'reason' => 'é–¢é€£æŠ€è¡“ã¨ã—ã¦æ³¨ç›®' }
            end
          }
        end
        all_keywords = trend_keywords + default_keywords.first(5)
      else
        # ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        puts "âš ï¸  ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ã«å¤±æ•—ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™"
        all_keywords = default_keywords
      end
    else
      # æ—¢å­˜ã®é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ä¸€éƒ¨ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
      existing_keywords = scored_interests.map { |s| s[:keyword].downcase }
      additional_defaults = default_keywords.reject do |d|
        existing_keywords.include?(d[:keyword].downcase)
      end
      all_keywords = scored_interests + additional_defaults.first(2)
    end
    
    # ä¸Šä½10å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¯¾è±¡ã«
    top_keywords = all_keywords.first(10)
    puts "\nğŸ“Š è¦³æ¸¬å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒˆãƒƒãƒ—10ï¼‰:"
    top_keywords.each_with_index do |interest, idx|
      puts "#{idx + 1}. #{interest[:keyword]} (ã‚¹ã‚³ã‚¢: #{interest[:total_score]})"
    end
    
    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
    all_observations = []
    
    top_keywords.each_with_index do |interest, idx|
      puts "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      puts "ğŸ” æ¤œç´¢ä¸­ (#{idx + 1}/#{top_keywords.length}): #{interest[:keyword]}"
      
      # é–¢é€£ãƒ¯ãƒ¼ãƒ‰ã‚‚å«ã‚ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
      search_query = build_search_query(interest)
      
      # Googleæ¤œç´¢å®Ÿè¡Œ
      articles = search_google_news(search_query, interest[:keyword])
      
      if articles.empty?
        puts "  â†’ æ–°ã—ã„è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        next
      end
      
      puts "  â†’ #{articles.length}ä»¶ã®è¨˜äº‹ã‚’ç™ºè¦‹"
      
      # GPTã§ä¾¡å€¤åˆ¤å®š
      valuable_articles = evaluate_articles_with_gpt(articles, interest)
      
      if valuable_articles.any?
        observation = {
          keyword: interest[:keyword],
          category: interest[:category],
          search_query: search_query,
          searched_at: Time.now.iso8601,
          total_found: articles.length,
          valuable_count: valuable_articles.length,
          articles: valuable_articles
        }
        
        all_observations << observation
        puts "  âœ… #{valuable_articles.length}ä»¶ã®ä¾¡å€¤ã‚ã‚‹è¨˜äº‹ã‚’ä¿å­˜"
        
        # è¦‹ãŸè¨˜äº‹ã¨ã—ã¦è¨˜éŒ²
        valuable_articles.each do |article|
          mark_as_seen(article[:url])
        end
      else
        puts "  â†’ ä¾¡å€¤ã‚ã‚‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
      end
      
      # APIåˆ¶é™å¯¾ç­–ã§å°‘ã—å¾…ã¤
      sleep(1)
    end
    
    # çµæœã‚’ä¿å­˜
    if all_observations.any?
      save_observations(all_observations)
      display_summary(all_observations)
    else
      puts "\nğŸ“­ æœ¬æ—¥ã¯æ–°ã—ã„ä¾¡å€¤ã‚ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    end
    
    # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_old_cache
  end
  
  private
  
  def build_search_query(interest)
    # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    query_parts = [interest[:keyword]]
    
    # é–¢é€£ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆæœ€å¤§3å€‹ï¼‰
    if interest[:related_hot_words] && interest[:related_hot_words].any?
      related = interest[:related_hot_words].first(3).map { |w| w['word'] }
      query_parts << "(" + related.join(" OR ") + ")"
    end
    
    # æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®š
    query_parts << "æœ€æ–°"
    
    query_parts.join(" ")
  end
  
  def search_google_news(query, keyword)
    return [] unless @google_api_key && @google_cx
    
    # éå»24æ™‚é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é™å®š
    date_restrict = "d1"
    
    uri = URI(GOOGLE_API_URL)
    params = {
      key: @google_api_key,
      cx: @google_cx,
      q: query,
      num: 10,
      dateRestrict: date_restrict,
      lr: 'lang_ja',  # æ—¥æœ¬èªã®çµæœ
      safe: 'active'
    }
    uri.query = URI.encode_www_form(params)
    
    response = Net::HTTP.get_response(uri)
    
    if response.code != '200'
      puts "  âŒ Google API Error: #{response.code} - #{response.body}"
      return []
    end
    
    data = JSON.parse(response.body)
    items = data['items'] || []
    
    # æ—¢ã«è¦‹ãŸè¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    new_items = items.reject { |item| already_seen?(item['link']) }
    
    new_items.map do |item|
      {
        title: item['title'],
        url: item['link'],
        snippet: item['snippet'],
        source: extract_source(item),
        keyword: keyword
      }
    end
  rescue => e
    puts "  âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: #{e.message}"
    []
  end
  
  def extract_source(item)
    # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º
    if item['displayLink']
      item['displayLink']
    elsif item['link']
      URI.parse(item['link']).host rescue 'unknown'
    else
      'unknown'
    end
  end
  
  def evaluate_articles_with_gpt(articles, interest)
    # è¨˜äº‹ã‚’ãƒãƒƒãƒã§GPTã«è©•ä¾¡ã—ã¦ã‚‚ã‚‰ã†
    prompt = build_evaluation_prompt(articles, interest)
    
    response = call_gpt_api(prompt)
    evaluations = parse_gpt_evaluation(response)
    
    return [] unless evaluations
    
    # ä¾¡å€¤ãŒã‚ã‚‹ã¨åˆ¤å®šã•ã‚ŒãŸè¨˜äº‹ã®ã¿æŠ½å‡º
    valuable_articles = []
    
    articles.each_with_index do |article, idx|
      eval_data = evaluations['articles'][idx] rescue nil
      next unless eval_data && eval_data['is_valuable']
      
      valuable_articles << {
        title: article[:title],
        url: article[:url],
        snippet: article[:snippet],
        source: article[:source],
        keyword: article[:keyword],
        evaluation: {
          relevance_score: eval_data['relevance_score'],
          novelty_score: eval_data['novelty_score'],
          importance_score: eval_data['importance_score'],
          reasoning: eval_data['reasoning'],
          key_points: eval_data['key_points']
        },
        evaluated_at: Time.now.iso8601
      }
    end
    
    # é‡è¤‡ã™ã‚‹å†…å®¹ã‚’é™¤å¤–
    deduplicate_articles(valuable_articles)
  end
  
  def build_evaluation_prompt(articles, interest)
    articles_text = articles.map.with_index do |article, idx|
      "è¨˜äº‹#{idx + 1}:
ã‚¿ã‚¤ãƒˆãƒ«: #{article[:title]}
URL: #{article[:url]}
è¦ç´„: #{article[:snippet]}
ã‚½ãƒ¼ã‚¹: #{article[:source]}"
    end.join("\n\n")
    
    <<~PROMPT
      ã‚ãªãŸã¯#{interest[:keyword]}ã«é–¢å¿ƒã®ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŸã‚ã®è¨˜äº‹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
      ä»¥ä¸‹ã®è¨˜äº‹ã‚’è©•ä¾¡ã—ã€ä¾¡å€¤ãŒã‚ã‚‹ã‹ã©ã†ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
      
      ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢å¿ƒäº‹:
      - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: #{interest[:keyword]}
      - ã‚«ãƒ†ã‚´ãƒª: #{interest[:category]}
      - æ–‡è„ˆ: #{interest[:context]}
      
      è©•ä¾¡ã™ã‚‹è¨˜äº‹:
      #{articles_text}
      
      ä»¥ä¸‹ã®åŸºæº–ã§å„è¨˜äº‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„:
      
      1. é–¢é€£æ€§ (1-10): ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®é–¢é€£åº¦
      2. æ–°è¦æ€§ (1-10): æ–°ã—ã„æƒ…å ±ã‚„è¦–ç‚¹ã‚’å«ã‚“ã§ã„ã‚‹ã‹
      3. é‡è¦æ€§ (1-10): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦çŸ¥ã‚‹ã¹ãé‡è¦ãªæƒ…å ±ã‹
      
      ä¾¡å€¤ãŒã‚ã‚‹è¨˜äº‹ã®æ¡ä»¶:
      - 3ã¤ã®ã‚¹ã‚³ã‚¢ã®å¹³å‡ãŒ7ä»¥ä¸Š
      - å˜ãªã‚‹æ—¢å­˜æƒ…å ±ã®ç¹°ã‚Šè¿”ã—ã§ã¯ãªã„
      - å®Ÿè³ªçš„ãªå†…å®¹ãŒã‚ã‚‹ï¼ˆå˜ãªã‚‹äºˆå‘Šã‚„å™‚ã§ã¯ãªã„ï¼‰
      
      JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
      {
        "articles": [
          {
            "index": è¨˜äº‹ç•ªå·,
            "is_valuable": true/false,
            "relevance_score": æ•°å€¤,
            "novelty_score": æ•°å€¤,
            "importance_score": æ•°å€¤,
            "reasoning": "åˆ¤å®šç†ç”±",
            "key_points": ["é‡è¦ãƒã‚¤ãƒ³ãƒˆ1", "é‡è¦ãƒã‚¤ãƒ³ãƒˆ2"],
            "duplicate_of": null ã¾ãŸã¯ä»–ã®è¨˜äº‹ç•ªå·
          }
        ],
        "summary": "å…¨ä½“çš„ãªè©•ä¾¡ã‚µãƒãƒªãƒ¼"
      }
    PROMPT
  end
  
  def call_gpt_api(prompt)
    uri = URI(OPENAI_API_URL)
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true
    http.read_timeout = 60
    
    request = Net::HTTP::Post.new(uri)
    request['Authorization'] = "Bearer #{@openai_api_key}"
    request['Content-Type'] = 'application/json'
    
    request.body = {
      model: @model,
      messages: [
        {
          role: 'system',
          content: 'è¨˜äº‹ã®ä¾¡å€¤ã‚’çš„ç¢ºã«è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.3,  # ã‚ˆã‚Šä¸€è²«æ€§ã®ã‚ã‚‹è©•ä¾¡ã®ãŸã‚ä½ã‚ã«
      max_tokens: 2000,
      response_format: { type: "json_object" }
    }.to_json
    
    response = http.request(request)
    JSON.parse(response.body)
  rescue => e
    puts "  âŒ GPT API Error: #{e.message}"
    nil
  end
  
  def parse_gpt_evaluation(response)
    return nil unless response
    
    if response['error']
      puts "  âŒ GPT Error: #{response['error']['message']}"
      return nil
    end
    
    content = response.dig('choices', 0, 'message', 'content')
    return nil unless content
    
    JSON.parse(content)
  rescue JSON::ParserError => e
    puts "  âŒ JSON Parse Error: #{e.message}"
    nil
  end
  
  def deduplicate_articles(articles)
    # é‡è¤‡åˆ¤å®šã®ãŸã‚ã€å„è¨˜äº‹ã®ç‰¹å¾´ã‚’æŠ½å‡º
    seen_signatures = {}
    unique_articles = []
    
    articles.each do |article|
      # ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚·ã‚°ãƒãƒãƒ£ã‚’ä½œæˆ
      signature = create_article_signature(article[:title])
      
      # æ—¢ã«ä¼¼ãŸè¨˜äº‹ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
      if seen_signatures[signature]
        puts "  â†’ é‡è¤‡: #{article[:title][0..50]}..."
        next
      end
      
      seen_signatures[signature] = true
      unique_articles << article
    end
    
    unique_articles
  end
  
  def create_article_signature(title)
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦ãªå˜èªã‚’æŠ½å‡ºã—ã¦ã‚·ã‚°ãƒãƒãƒ£ã‚’ä½œæˆ
    # ç°¡æ˜“çš„ãªå®Ÿè£…
    words = title.gsub(/[ã€Œã€ã€ã€ã€ã€‘\[\]()ï¼ˆï¼‰]/, ' ')
                 .split(/[\sã€ã€‚ãƒ»]+/)
                 .reject { |w| w.length < 2 }
                 .first(5)
    
    Digest::MD5.hexdigest(words.join('_').downcase)
  end
  
  def already_seen?(url)
    @seen_articles[url] ? true : false
  end
  
  def mark_as_seen(url)
    @seen_articles[url] = Time.now.iso8601
    save_seen_articles
  end
  
  def load_seen_articles
    return {} unless File.exist?(@seen_articles_file)
    JSON.parse(File.read(@seen_articles_file))
  rescue
    {}
  end
  
  def save_seen_articles
    File.write(@seen_articles_file, JSON.pretty_generate(@seen_articles))
  end
  
  def cleanup_old_cache
    # 30æ—¥ä»¥ä¸Šå‰ã®è¨˜éŒ²ã‚’å‰Šé™¤
    cutoff_date = Date.today - 30
    
    @seen_articles.delete_if do |url, timestamp|
      Date.parse(timestamp) < cutoff_date rescue false
    end
    
    save_seen_articles
  end
  
  def save_observations(observations)
    timestamp = Time.now.strftime('%Y%m%d_%H%M%S')
    filename = "daily_observation_#{timestamp}.json"
    filepath = File.join(OUTPUT_DIR, filename)
    
    # ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
    daily_summary = generate_daily_summary(observations)
    
    data = {
      observed_at: Time.now.iso8601,
      total_keywords: observations.length,
      total_valuable_articles: observations.sum { |o| o[:valuable_count] },
      observations: observations,
      daily_summary: daily_summary
    }
    
    File.write(filepath, JSON.pretty_generate(data))
    puts "\nğŸ’¾ è¦³æ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: #{filepath}"
    
    # æœ€æ–°ç‰ˆã‚‚ä¿å­˜
    latest_path = File.join(OUTPUT_DIR, 'latest_observation.json')
    File.write(latest_path, JSON.pretty_generate(data))
  end
  
  def generate_daily_summary(observations)
    puts "\nğŸ“ ãƒ‡ã‚¤ãƒªãƒ¼ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­..."
    
    # ä¾¡å€¤ã‚ã‚‹è¨˜äº‹ã ã‘ã‚’æŠ½å‡º
    valuable_articles = []
    observations.each do |obs|
      obs[:new_articles].each do |article|
        if article[:relevance_score] >= 7
          valuable_articles << {
            keyword: obs[:keyword],
            title: article[:title],
            url: article[:url],
            snippet: article[:snippet],
            score: article[:relevance_score]
          }
        end
      end
    end
    
    return nil if valuable_articles.empty?
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    articles_text = valuable_articles.map do |a|
      "ã€#{a[:keyword]}ã€‘#{a[:title]} (ã‚¹ã‚³ã‚¢: #{a[:score]})\n#{a[:snippet]}"
    end.join("\n\n")
    
    prompt = <<~PROMPT
      ä»¥ä¸‹ã¯æœ¬æ—¥ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰è¦³æ¸¬ã§ç™ºè¦‹ã•ã‚ŒãŸé‡è¦ãªè¨˜äº‹ã§ã™ã€‚
      
      #{articles_text}
      
      ã“ã‚Œã‚‰ã®è¨˜äº‹ã‹ã‚‰ä»¥ä¸‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
      1. æœ¬æ—¥ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¦ç´„ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰
      2. ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒæ³¨ç›®ã™ã¹ããƒã‚¤ãƒ³ãƒˆï¼ˆ3ã¤ï¼‰
      3. æ˜æ—¥ä»¥é™ã®å‹•å‘äºˆæ¸¬ï¼ˆç°¡æ½”ã«ï¼‰
      
      ç°¡æ½”ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
    PROMPT
    
    # GPTã§ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
    summary = generate_summary_with_gpt(prompt)
    
    {
      generated_at: Time.now.iso8601,
      valuable_articles_count: valuable_articles.length,
      summary: summary,
      top_articles: valuable_articles.first(5)
    }
  rescue => e
    puts "âš ï¸  ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: #{e.message}"
    nil
  end
  
  def generate_summary_with_gpt(prompt)
    uri = URI(OPENAI_API_URL)
    
    request = Net::HTTP::Post.new(uri)
    request['Authorization'] = "Bearer #{@openai_api_key}"
    request['Content-Type'] = 'application/json'
    
    request.body = {
      model: @model,
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 500
    }.to_json
    
    response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
      http.request(request)
    end
    
    if response.code == '200'
      data = JSON.parse(response.body)
      data.dig('choices', 0, 'message', 'content')
    else
      raise "GPT API Error: #{response.body}"
    end
  end
  
  def display_summary(observations)
    puts "\nğŸ“Š æœ¬æ—¥ã®å®šç‚¹è¦³æ¸¬ã‚µãƒãƒªãƒ¼"
    puts "="*50
    
    total_articles = observations.sum { |o| o[:valuable_count] }
    puts "âœ… ç™ºè¦‹ã—ãŸä¾¡å€¤ã‚ã‚‹è¨˜äº‹: #{total_articles}ä»¶"
    
    puts "\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥:"
    observations.each do |obs|
      next if obs[:valuable_count] == 0
      puts "\nã€#{obs[:keyword]}ã€‘ #{obs[:valuable_count]}ä»¶"
      obs[:articles].each do |article|
        puts "  - #{article[:title][0..60]}..."
        puts "    #{article[:source]} | é–¢é€£æ€§:#{article[:evaluation][:relevance_score]} æ–°è¦æ€§:#{article[:evaluation][:novelty_score]} é‡è¦æ€§:#{article[:evaluation][:importance_score]}"
      end
    end
  end
end

# å®Ÿè¡Œ
if __FILE__ == $0
  unless ENV['GOOGLE_API_KEY'] && ENV['GOOGLE_CUSTOM_SEARCH_CX'] && ENV['OPENAI_API_KEY']
    puts "âŒ Error: å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
    puts "å¿…è¦ãªç’°å¢ƒå¤‰æ•°:"
    puts "  - GOOGLE_API_KEY"
    puts "  - GOOGLE_CUSTOM_SEARCH_CX"
    puts "  - OPENAI_API_KEY"
    exit 1
  end
  
  observer = DailyInterestObserver.new
  observer.run_daily_observation
end