#!/usr/bin/env ruby

require 'json'
require 'time'
require 'openai'
require 'dotenv'
require_relative 'raindrop_client'
require_relative 'interest_manager'
require_relative 'article_content_fetcher'
require_relative 'gpt_keyword_extractor'

Dotenv.load('/var/git/rainpipe/.env')

class WeeklySummaryGenerator
  SUMMARY_DIR = './data/weekly_summaries'
  
  def initialize
    @client = RaindropClient.new
    @interest_manager = InterestManager.new
    @content_fetcher = ArticleContentFetcher.new
    @openai = OpenAI::Client.new(access_token: ENV['OPENAI_API_KEY'])
    
    FileUtils.mkdir_p(SUMMARY_DIR) unless Dir.exist?(SUMMARY_DIR)
  end
  
  def generate_weekly_summary(week_start_date)
    puts "ğŸ“Š é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­: #{week_start_date}"
    
    # 1. ãã®é€±ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’å–å¾—
    week_end_date = Date.parse(week_start_date) + 6
    bookmarks = @client.get_bookmarks_by_date_range(week_start_date, week_end_date.to_s)
    
    puts "ğŸ“š #{bookmarks.length}ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’å–å¾—"
    
    # 2. é–¢å¿ƒãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
    latest_analysis = @interest_manager.get_latest_analysis
    keywords = extract_active_keywords(latest_analysis)
    
    puts "ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: #{keywords.join(', ')}"
    
    # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    grouped_bookmarks = group_bookmarks_by_keywords(bookmarks, keywords)
    
    # 4. å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¦ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
    summary_data = {
      week_start: week_start_date,
      week_end: week_end_date.to_s,
      generated_at: Time.now.iso8601,
      keywords: {}
    }
    
    grouped_bookmarks.each do |keyword, keyword_bookmarks|
      puts "\nğŸ” #{keyword}ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­..."
      
      # è¨˜äº‹å†…å®¹ã‚’å–å¾—
      articles_with_content = fetch_articles_content(keyword_bookmarks)
      
      # ã‚µãƒãƒªãƒ¼ã¨æ´å¯Ÿã‚’ç”Ÿæˆ
      if articles_with_content.any?
        summary_data[:keywords][keyword] = generate_keyword_summary(keyword, articles_with_content)
      end
    end
    
    # 5. å…¨ä½“ã®ç·æ‹¬ã‚’ç”Ÿæˆ
    summary_data[:overall_insights] = generate_overall_insights(summary_data[:keywords])

    # 6. å‘¨è¾ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆrelated_clustersï¼‰ã‚’æŠ½å‡º
    begin
      puts "\nğŸ” å‘¨è¾ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­..."
      week_key = "#{week_start_date}ï½#{week_end_date}"
      extractor = GPTKeywordExtractor.new
      analysis = extractor.extract_keywords_from_bookmarks(bookmarks, week_key)

      if analysis && analysis['related_clusters']
        summary_data[:related_clusters] = analysis['related_clusters']
        puts "âœ“ #{analysis['related_clusters'].length}å€‹ã®å‘¨è¾ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ"
      end
    rescue => e
      puts "âš ï¸  å‘¨è¾ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—: #{e.message}"
    end

    # 7. çµæœã‚’ä¿å­˜
    save_summary(week_start_date, summary_data)
    
    summary_data
  end
  
  private
  
  def extract_active_keywords(analysis)
    return [] unless analysis
    
    keywords = []
    
    # core_interestsã‹ã‚‰æŠ½å‡º
    core_interests = analysis.dig('analysis', 'core_interests') || []
    keywords.concat(core_interests.map { |i| i['keyword'] })
    
    # emerging_interestsã‹ã‚‰æŠ½å‡º
    emerging_interests = analysis.dig('analysis', 'emerging_interests') || []
    keywords.concat(emerging_interests.map { |i| i['keyword'] })
    
    keywords.uniq
  end
  
  def group_bookmarks_by_keywords(bookmarks, keywords)
    grouped = {}
    
    keywords.each do |keyword|
      keyword_lower = keyword.downcase
      related_bookmarks = bookmarks.select do |bookmark|
        title = (bookmark['title'] || '').downcase
        tags = bookmark['tags'] || []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯ã‚¿ã‚°ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹
        title.include?(keyword_lower) || 
        tags.any? { |tag| tag.downcase.include?(keyword_lower) }
      end
      
      grouped[keyword] = related_bookmarks if related_bookmarks.any?
    end
    
    grouped
  end
  
  def fetch_articles_content(bookmarks)
    articles = []
    
    bookmarks.first(5).each do |bookmark|  # æœ€å¤§5è¨˜äº‹ã¾ã§
      url = bookmark['link']
      next unless url
      
      begin
        content_data = @content_fetcher.fetch_content(url)
        articles << {
          title: bookmark['title'],
          url: url,
          content: content_data,
          created_at: bookmark['created']
        }
      rescue => e
        puts "âš ï¸  è¨˜äº‹å–å¾—ã‚¹ã‚­ãƒƒãƒ—: #{e.message}"
      end
    end
    
    articles
  end
  
  def generate_keyword_summary(keyword, articles)
    return nil if articles.empty?
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    articles_text = articles.map.with_index do |article, idx|
      content_text = article[:content] && article[:content][:text] ? article[:content][:text][0..500] : "å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
      "è¨˜äº‹#{idx + 1}: #{article[:title]}\nå†…å®¹: #{content_text}..."
    end.join("\n\n")
    
    prompt = <<~PROMPT
      ä»¥ä¸‹ã¯ã€Œ#{keyword}ã€ã«é–¢ã™ã‚‹ä»Šé€±ã®è¨˜äº‹ã§ã™ã€‚
      
      #{articles_text}
      
      ã“ã‚Œã‚‰ã®è¨˜äº‹ã‹ã‚‰ä»¥ä¸‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
      
      1. ä»Šé€±ã®ä¸»è¦ãªå‹•å‘ï¼ˆ3-5ã¤ã®ç®‡æ¡æ›¸ãï¼‰
      2. æŠ€è¡“çš„ãªé‡è¦ãƒã‚¤ãƒ³ãƒˆ
      3. å®Ÿç”¨çš„ãªæ´å¯Ÿï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒçŸ¥ã£ã¦ãŠãã¹ãã“ã¨ï¼‰
      4. æ¥é€±ä»¥é™ã®æ³¨ç›®ç‚¹
      
      ç°¡æ½”ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
    PROMPT
    
    begin
      response = @openai.chat(
        parameters: {
          model: "gpt-4o-mini",
          messages: [{ role: "user", content: prompt }],
          temperature: 0.7,
          max_tokens: 800
        }
      )
      
      summary_text = response.dig("choices", 0, "message", "content")
      
      {
        article_count: articles.length,
        articles: articles.map { |a| { title: a[:title], url: a[:url] } },
        summary: summary_text,
        generated_at: Time.now.iso8601
      }
    rescue => e
      puts "âŒ ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: #{e.message}"
      nil
    end
  end
  
  def generate_overall_insights(keywords_data)
    return nil if keywords_data.empty?
    
    keywords_summary = keywords_data.map do |keyword, data|
      "#{keyword}: #{data[:article_count]}è¨˜äº‹"
    end.join(", ")
    
    prompt = <<~PROMPT
      ä»Šé€±ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚µãƒãƒªãƒ¼:
      #{keywords_summary}
      
      å…¨ä½“çš„ãªæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ´å¯Ÿã‚’200æ–‡å­—ç¨‹åº¦ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
      ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒä»Šé€±æ³¨ç›®ã™ã¹ããƒã‚¤ãƒ³ãƒˆã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
    PROMPT
    
    begin
      response = @openai.chat(
        parameters: {
          model: "gpt-4o-mini",
          messages: [{ role: "user", content: prompt }],
          temperature: 0.7,
          max_tokens: 300
        }
      )
      
      response.dig("choices", 0, "message", "content")
    rescue => e
      puts "âŒ ç·æ‹¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: #{e.message}"
      nil
    end
  end
  
  def save_summary(week_start_date, summary_data)
    filename = File.join(SUMMARY_DIR, "summary_#{week_start_date}.json")
    File.write(filename, JSON.pretty_generate(summary_data))
    
    # æœ€æ–°ç‰ˆã‚‚ä¿å­˜
    latest_file = File.join(SUMMARY_DIR, "latest.json")
    File.write(latest_file, JSON.pretty_generate(summary_data))
    
    puts "âœ… ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: #{filename}"
  end
end

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __FILE__ == $0
  generator = WeeklySummaryGenerator.new
  
  # ä»Šé€±ã®æœˆæ›œæ—¥ã‚’è¨ˆç®—
  today = Date.today
  monday = today - (today.wday - 1) % 7
  
  puts "ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: #{monday}ã®é€±ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"
  summary = generator.generate_weekly_summary(monday.to_s)
  
  if summary
    puts "\nâœ… ã‚µãƒãƒªãƒ¼ç”Ÿæˆå®Œäº†"
    puts "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: #{summary[:keywords].keys.length}"
  end
end